#
# Copyright 2018 Delphix
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

#
# This file should not be directly modified by support, as these changes
# would be removed on upgrade. In order to make custom tunable changes
# for specific customer installations, please edit the file (creating it
# if necessary): /etc/modprobe.d/20-zfs.conf
#

#
# Panic on ZFS assertion failures; otherwise it's possible for these
# assertion failures to only halt the thread that failed the assert,
# which can then put the entire system into a "hung" state, since we're
# running a rootfs on ZFS.
#
options spl spl_panic_halt=1

#
# We increase the max memory limit for ZFS channel programs to 512MB to
# give us a wide buffer for development of channel programs. We were
# using upwards of 70MB during internal testing, which is too much with
# the default limit of 100MB.
#
options zfs zfs_lua_max_memlimit=536870912

#
# On Linux, SEEK_FILE and SEEK_HOLE are inaccurate if the file is dirty;
# in which case, they say that there are no holes. To get the same
# behavior as illumos, we need to configure this tunable.
#
options zfs zfs_dmu_offset_next_sync=1

#
# We require these settings to maintain parity with our illumos based
# appliance w.r.t. read and write performance. We originally tuned these
# values on our illumos based appliance due to reports of poor
# performance on customers deployments, when using the default values.
#
options zfs zfs_vdev_sync_read_min_active=35
options zfs zfs_vdev_sync_read_max_active=35
options zfs zfs_vdev_sync_write_min_active=35
options zfs zfs_vdev_sync_write_max_active=35

#
# This setting is made to boost the performance ZFS replication, by
# allowing more async reads to be inflight at any given time.
#
options zfs zfs_vdev_async_read_max_active=10

#
# The default value used for this tunable is configured such that it
# factors in things like RAID-Z and dedup. DelphixOS does not use RAID-Z
# or dedup, so we can optimize this value better for our particular
# workload. The worst case is 3 copies in the DVAs, and one extra copy
# for good luck (e.g. ganging).
#
options zfs spa_asize_inflation=4

#
# The following changes are made to match the values for these same
# tunables, that we use on our illumos-based DelphixOS.
#
options zfs zfs_mg_noalloc_threshold=5

#
# Our data blocks are of size 8k, but the filesystems usually have a
# recordsize of 128k. That causes incorrect size estimates in zfs send.
# Overriding the property here fixes that.
#
options zfs zfs_override_estimate_recordsize=8192

#
# The default arc metadata eviction policy is a balanced policy. This policy
# evicts both data and metadata and provides a callback to the ZPL layer
# to evict blocks from the inode and dentry caches. This policy has serious
# implications because it tries too hard to reduce metadata so that it's
# below the arc_meta_limit. As a result this can end up consuming all of
# the cpus and causing the system to run out of memory. Since the arc
# metadata eviction is called in the critical path, we want a lightweight
# policy to be used to ensure that eviction happens quickly and new additions
# to the arc are not blocked for a significant amount of time. To do this,
# we changes the metadata eviction policy to ARC_STRATEGY_META_ONLY,
# which is what we used in illumos.
#
options zfs zfs_arc_meta_strategy=0

#
# Increasing zvol taskq threads to 256 to improve performance for some
# workloads. Taskq threads are created and destroyed dynamically so there
# should be minimal to no impact on small systems.
#
options zfs zvol_threads=256
