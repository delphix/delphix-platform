#
# Copyright 2018 Delphix
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

---
#
# We use a non-standard directory for the appliance user's home
# directory. As a result, we have to explicitly create the "base
# directory" here, rather than rely on Ansible's user module to create
# it below; otherwise that task will fail.
#
- file:
    path: /export/home
    state: directory
    mode: 0755

- user:
    name: delphix
    uid: 65433
    group: staff
    groups: root
    shell: /bin/bash
    create_home: yes
    comment: Delphix User
    home: /export/home/delphix

#
# In order for this locale to be used (e.g. by virtualization) we need
# to ensure it's available. Generally this locale will be generated by
# appliance-build for new VMs, but we can't rely on that since that
# doesn't work for not-in-place upgrades. Thus, we must explicitly
# generate the locale here.
#
- locale_gen:
    name: en_US.UTF-8
    state: present

#
# If we don't disable the "OS prober" script, when we update the grub
# configuration, it may find OS installations that we don't want. For
# example, when updating grub from within the chroot environment, it may
# find the OS installed on the build server's disks, since we bind mount
# the host's /dev directory into the chroot environment. By disabling
# the "OS prober" script, we avoid this issue.
#
- file:
    path: /etc/grub.d/30_os-prober
    mode: 0644

#
# We want the quota on the crashdump dataset to be sized accordingly
# based on the capacity of rpool. Thus, we use this command to query the
# rpool size, and then use this value later to set the crashdump quota.
#
- command: zpool list -Hpo size rpool
  register: rpool_size_bytes
  when: not ansible_is_chroot

#
# Reconfigure the crashdump dataset. The initial configuration was performed
# by appliance build so we only update the configuration here. In particular,
# we want to enforce a quota to ensure its contents can't run rpool out
# of space.
#
- zfs:
    name: rpool/crashdump
    state: present
    extra_zfs_properties:
      quota: "{{ rpool_size_bytes.stdout|int / 2 }}"
  when: not ansible_is_chroot

#
# Create a world writeable directory for application and kernel core
# dumps. We want it world writeable because we're sharing one directory
# for corefiles from any user. Unlike illumos where all appliacation
# cores are written out as the root user, linux cores are written with
# the UID of the running process.
#
- file:
    path: /var/crash
    state: directory
    mode: 0777

#
# This directory is used by upgrade; it's bind mounted into the upgrade
# container such that software can write files to this directory, and
# these files can easily be accessed by software running on the host.
#
- file:
    path: /var/log/delphix-upgrade
    state: directory

#
# Create the directory and ZFS dataset that we'll use to store unpacked
# upgrade images. This directory is used by the upgrade related scripts
# found in this directory, but also used by upgrade-scripts stored in
# the appliace-build repository (which generates the upgrade image).
# Thus, we need to be careful if/when changing this, as we'll need to
# coordinate the change with the appliance-build upgrade-scripts.
#
- file:
    path: /var/dlpx-update
    state: directory

#
# The zfs module cannot be run from the chroot environment that's used
# by appliance-build. Thus, we disable this when run in that context by
# only running this when ansible_is_chroot is not true.
#
- zfs:
    name: rpool/update
    state: present
    extra_zfs_properties:
      mountpoint: /var/dlpx-update
      compression: gzip
      #
      # internal-qa migration images are currently 5G in size and internal-dev
      # are 11.5G. We need double that space as the uploaded image is unpacked
      # in the same dataset, thus requiring a minimum of 23G. We use 30G to
      # leave a safety margin of a few GBs.
      #
      quota: 30g
  when: not ansible_is_chroot

#
# Configure command audit logging
#
# We want to record all commands executed on the appliance. Opt out for
# setsid since all ExecuteUtils.execute wrap each call with setsid.
#
- lineinfile:
    path: /etc/audit/auditd.conf
    regexp: "{{ item.regex }}"
    line: "{{ item.line }}"
  with_items:
    - { regex: "^num_logs =", line: "num_logs = 6" }
    - { regex: "^max_log_file =", line: "max_log_file = 3072" }
    - { regex: "^max_log_file_action =", line: "max_log_file_action = rotate" }
    - { regex: "^log_format =", line: "log_format = RAW" }

- blockinfile:
    path: /etc/audit/rules.d/audit.rules
    insertafter: EOF
    block: |
      ## Record all executed commands (excluding setsid)
      -a exit,never -S execve -F exe=/usr/bin/setsid
      -a exit,always -S execve

      ## Record command exit failures (execve result is not command result)
      -a exit,always -F a0!=0 -S exit_group
      -a exit,always -F a0!=0 -S exit

#
# Prevent auditd output from being duplicated into the journal. The size
# of the journal is limited and the high volume of messages from auditd
# would end up causing important messages from less verbose services to
# be flushed and lost prematurely.
#
- shell: systemctl mask systemd-journald-audit.socket

#
# By default, the ulimit for core files is set to 0, and the default
# filename and location for a core file is 'core' in the cwd. Update
# limits.conf to allow processes running as root or a regular user to
# make core files.
#
- lineinfile:
    create: yes
    path: /etc/security/limits.conf
    line: "{{ item }} soft core unlimited"
  with_items:
    - 'root'
    - '*'

- lineinfile:
    path: /etc/ssh/sshd_config
    regexp: "^#?{{ item.key }} "
    line: "{{ item.key }} {{ item.value }}"
  with_items:
    #
    # Configure SSH to allow PAM "conversations" (interactions with the user).
    #
    - { key: "ChallengeResponseAuthentication", value: "yes" }
    #
    # Harden the appliance by disabling ssh-agent(1), tcp, UNIX domain, and
    # X11 forwarding. Note that this doesn't improve security unless users are
    # also denied shell access.
    #
    - { key: "AllowAgentForwarding", value: "no" }
    - { key: "AllowStreamLocalForwarding", value: "no" }
    - { key: "AllowTcpForwarding", value: "no" }
    - { key: "X11Forwarding", value: "no" }
    #
    # The CRA project mandated a 30 minute timeout for any idle connections.
    # By enabling an inactivity timeout we ensure that idle connections are
    # closed. Thus any sessions that are accidentally left opened at a
    # customer site will timeout preventing customers from gaining access
    # to our engine.
    #
    - { key: "ClientAliveInterval", value: "1800" }
    - { key: "ClientAliveCountMax", value: "0" }

#
# Harden the appliance by disabling SFTP.
#
- replace:
    path: /etc/ssh/sshd_config
    regexp: '^(Subsystem.*sftp.*)'
    replace: '#\1'

#
# Ssh leads to the CLI, not bash, so let's remove all the linuxy shell goodies,
# like last-login, "welcome to ubuntu", and help messages. This makes linux and
# illumos look the same, too.
#
- replace:
    dest: /etc/ssh/sshd_config
    regexp: '^#?[\s]*PrintLastLog.*$'
    replace: 'PrintLastLog no'
- replace:
    dest: /etc/pam.d/sshd
    regexp: '^(session[\s]+optional[\s]+pam_motd\.so.*)$'
    replace: '#\1'

#
# Enable SNMP client tools to load MIBs by default.
#
- replace:
    path: /etc/snmp/snmp.conf
    regexp: '^(mibs\s+:\s+)'
    replace: '#\1'

#
# During upgrade verification (using the "verify" upgrade script), we'll
# use this "dropbox" directory to bind mount into the verification
# container, such that software running in that container can use this
# directory to communicate results of the verification back to the host.
#
# This path is also found in "common.sh" of the "upgrade-scripts", as
# well as other Delphix repositories. Thus, we need to be careful when
# changing this, as we may also need to update these other places where
# this path is referenced.
#
- file:
    path: /var/delphix/dropbox
    state: directory
    mode: 0755
    recurse: yes

- lineinfile:
    path: /etc/environment
    regexp: '^{{ item.key }}='
    line: '{{ item.key }}="{{ item.value }}"'
  with_items:
    - { key: 'JAVA_HOME', value: '/usr/lib/jvm/adoptopenjdk-java8-jdk-amd64' }

#
# Configure the Azure agent. Only run this on Azure, since that is the
# only platform that has the Azure agent installed.
#
- lineinfile:
    path: /etc/waagent.conf
    regexp: '^{{ item.key }}='
    line: '{{ item.key }}={{ item.value }}'
  with_items:
    #
    # We use cloud-init rather than the Azure agent to handle any
    # provisioning logic that we need.
    #
    - { key: 'Provisioning.Enabled', value: 'n' }
    #
    # Even though we do use cloud-init to handle some provisioning
    # tasks, we should tell the Azure agent that we don't. Otherwise,
    # the Azure agent will wait for cloud-init to complete before
    # reporting back to Azure that the VM is running. The way it detects
    # that cloud-init has finished is by waiting for cloud-init to copy
    # a certain file (ovf-env.xml) into /var/lib/waagent/, but
    # cloud-init won't copy this file unless it has been configured to
    # read Azure userdata (as determined by the 'datasource_list'
    # parameter). We do not allow the appliance to read userdata from
    # any source, at least on the external variant, with the result that
    # the agent waits 20+ minutes before timing out and finally
    # reporting the VM as running.
    #
    - { key: 'Provisioning.UseCloudInit', value: 'n' }
    #
    # Prevent customers from running arbitrary code on the engine via
    # extensions when they deploy.
    #
    - { key: 'Extensions.Enabled', value: 'n' }
    #
    # This controls auto-updating of the extension handler (not the
    # provisioning handler or daemon). It seems safer to disable this
    # rather than to allow Microsoft to push new code to our engines,
    # especially since we aren't using any extensions.
    #
    - { key: 'AutoUpdate.Enabled', value: 'n' }
  when: platform == "azure"

#
# Customize the GCP linux environment.
#
# Update the override file for the GCP instance. This file gets
# applied dynamically by running google_instance_setup script.
#
- blockinfile:
    path: /etc/default/instance_configs.cfg.template
    create: yes
    block: |
      #
      # Disable the accounts daemon to prevent adding/removing
      # users on the engine.
      #
      [Daemons]
      accounts_daemon = false

      #
      # Disable user supplied startup/shutdown scripts from running on
      # the engine.
      #
      [MetadataScripts]
      shutdown = false
      startup = false
  when:
    - platform == "gcp"
  notify: "gcp config changed"

#
# Make sure that the account daemon is always disabled. The override file
# above should prevent this and this is designed to catch any corner cases.
#
- command: systemctl disable google-accounts-daemon.service
  when:
    - platform == "gcp"
